{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import thr required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Declare the directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory :  c:\\Users\\Avishka Sandeepa\\OneDrive - University of Moratuwa\\GitHub\\Traffic-Signs-Recognition\\main\\codes\n",
      "Current working directory :  C:\\Users\\Avishka Sandeepa\\OneDrive - University of Moratuwa\\GitHub\\Traffic-Signs-Recognition\\main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print (\"Current working directory : \" , current_directory)\n",
    "\n",
    "# define the directory of data set\n",
    "data_direcory = \"C:/Users/Avishka Sandeepa/OneDrive - University of Moratuwa/GitHub/Traffic-Signs-Recognition/main\"\n",
    "# chane the current directory to the data directory\n",
    "os.chdir(data_direcory)\n",
    "\n",
    "# chek the working directory again after changing\n",
    "current_directory = os.getcwd()\n",
    "print (\"Current working directory : \" , current_directory)\n",
    "\n",
    "# define the each folder directories for training and testing data\n",
    "train_set = 'Train'\n",
    "test_set = 'Test'\n",
    "\n",
    "num_classes = len(os.listdir(train_set)) # returns the no of classes inside the training folder(0 to 42)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preprocessing the Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acess the each and every image\n",
    "\n",
    "img_data =[]\n",
    "img_labels = []\n",
    " \n",
    "for index in range(num_classes):   # address all classes  \n",
    "    path = os.path.join(current_directory,'Train',str(index)) # create the path for each image    \n",
    "    image_names = os.listdir(path) # list containing names of all images for a running class at that point\n",
    "    \n",
    "    \n",
    "    for image in image_names:\n",
    "        img = cv.imread(path + '/' + image, cv.IMREAD_COLOR)\n",
    "        img = cv.resize(img, (32,32)) # resizing all images to one scale\n",
    "        # convert image data into numy array\n",
    "        img = np.array(img) \n",
    "\n",
    "        img_data.append(img)\n",
    "        img_labels.append(index)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39209, 32, 32, 3) (39209,)\n"
     ]
    }
   ],
   "source": [
    "# lets convert above created list into numpy arrays that helps to model to training\n",
    "\n",
    "img_data = np.array(img_data)\n",
    "img_labels = np.array(img_labels)\n",
    "\n",
    "print(img_data.shape, img_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape (26270, 32, 32, 3)\n",
      "val_x.shape (12939, 32, 32, 3)\n",
      "train_y.shape (26270,)\n",
      "val_y.shape (12939,)\n"
     ]
    }
   ],
   "source": [
    "# Split into training and validation data\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(img_data, img_labels, test_size=0.33, random_state=42, shuffle=True)\n",
    "train_x = train_x/255 \n",
    "val_x = val_x/255\n",
    "\n",
    "print(\"train_x.shape\", train_x.shape)\n",
    "print(\"val_x.shape\", val_x.shape)\n",
    "print(\"train_y.shape\", train_y.shape)\n",
    "print(\"val_y.shape\", val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y.shape (26270, 43)\n",
      "val_y.shape (12939, 43)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#This function returns a matrix of binary values (either ‘1’ or ‘0’). It has number of rows equal to the length of\n",
    "#the input vector and number of columns equal to the number of classes.\n",
    "train_y = tf.keras.utils.to_categorical(train_y, num_classes=num_classes)\n",
    "val_y = tf.keras.utils.to_categorical(val_y, num_classes=num_classes)\n",
    "\n",
    "print(\"train_y.shape\", train_y.shape)\n",
    "print(\"val_y.shape\", val_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Augmenting the image data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    fill_mode=\"nearest\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Build the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 16)        448       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 12, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 10, 10, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1638912   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 43)                22059     \n",
      "=================================================================\n",
      "Total params: 1,758,411\n",
      "Trainable params: 1,758,411\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), activation = \"relu\", input_shape=(32,32,3)))\n",
    "model.add(Conv2D(32, (3, 3), activation = \"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # maxpooling to reduce the dimentions\n",
    "model.add(Dropout(rate=0.25)) # Add dropout to prevent overfitting \n",
    "\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation = \"relu\"))\n",
    "model.add(Conv2D(128, (3, 3), activation = \"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.25)) # Add dropout to prevent overfitting \n",
    "\n",
    "\n",
    "# Flatten serves as a connection between the convolution and dense layers. \n",
    "model.add(Flatten()) \n",
    "model.add(Dense(512, activation = \"relu\"))\n",
    "model.add(Dropout(rate=0.5)) # Add dropout to prevent overfitting \n",
    "\n",
    "# since we have 43 classes add dense with 43 \n",
    "model.add(Dense(43, activation = \"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python38\\cv\\lib\\site-packages\\keras\\backend.py:4846: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526/526 [==============================] - 81s 152ms/step - loss: 2.0781 - accuracy: 0.3925 - val_loss: 0.5216 - val_accuracy: 0.8352\n",
      "Epoch 2/50\n",
      "526/526 [==============================] - 71s 134ms/step - loss: 0.6409 - accuracy: 0.7974 - val_loss: 0.0974 - val_accuracy: 0.9808\n",
      "Epoch 3/50\n",
      "526/526 [==============================] - 73s 139ms/step - loss: 0.3236 - accuracy: 0.8990 - val_loss: 0.0434 - val_accuracy: 0.9888\n",
      "Epoch 4/50\n",
      "526/526 [==============================] - 79s 150ms/step - loss: 0.2202 - accuracy: 0.9322 - val_loss: 0.0248 - val_accuracy: 0.9948\n",
      "Epoch 5/50\n",
      "526/526 [==============================] - 98s 186ms/step - loss: 0.1689 - accuracy: 0.9471 - val_loss: 0.0174 - val_accuracy: 0.9960\n",
      "Epoch 6/50\n",
      "526/526 [==============================] - 87s 166ms/step - loss: 0.1357 - accuracy: 0.9572 - val_loss: 0.0155 - val_accuracy: 0.9949\n",
      "Epoch 7/50\n",
      "526/526 [==============================] - 86s 164ms/step - loss: 0.1189 - accuracy: 0.9636 - val_loss: 0.0094 - val_accuracy: 0.9976\n",
      "Epoch 8/50\n",
      "526/526 [==============================] - 83s 158ms/step - loss: 0.1052 - accuracy: 0.9679 - val_loss: 0.0087 - val_accuracy: 0.9976\n",
      "Epoch 9/50\n",
      "526/526 [==============================] - 83s 157ms/step - loss: 0.0916 - accuracy: 0.9731 - val_loss: 0.0085 - val_accuracy: 0.9976\n",
      "Epoch 10/50\n",
      "526/526 [==============================] - 74s 141ms/step - loss: 0.0888 - accuracy: 0.9722 - val_loss: 0.0139 - val_accuracy: 0.9969\n",
      "Epoch 11/50\n",
      "526/526 [==============================] - 76s 144ms/step - loss: 0.0744 - accuracy: 0.9764 - val_loss: 0.0093 - val_accuracy: 0.9968\n",
      "Epoch 12/50\n",
      "526/526 [==============================] - 76s 145ms/step - loss: 0.0704 - accuracy: 0.9773 - val_loss: 0.0073 - val_accuracy: 0.9983\n",
      "Epoch 13/50\n",
      "526/526 [==============================] - 77s 147ms/step - loss: 0.0698 - accuracy: 0.9790 - val_loss: 0.0113 - val_accuracy: 0.9968\n",
      "Epoch 14/50\n",
      "526/526 [==============================] - 77s 147ms/step - loss: 0.0644 - accuracy: 0.9798 - val_loss: 0.0064 - val_accuracy: 0.9980\n",
      "Epoch 15/50\n",
      "526/526 [==============================] - 77s 146ms/step - loss: 0.0643 - accuracy: 0.9805 - val_loss: 0.0051 - val_accuracy: 0.9986\n",
      "Epoch 16/50\n",
      "526/526 [==============================] - 77s 146ms/step - loss: 0.0613 - accuracy: 0.9819 - val_loss: 0.0061 - val_accuracy: 0.9981\n",
      "Epoch 17/50\n",
      "526/526 [==============================] - 77s 147ms/step - loss: 0.0637 - accuracy: 0.9795 - val_loss: 0.0048 - val_accuracy: 0.9988\n",
      "Epoch 18/50\n",
      "526/526 [==============================] - 77s 146ms/step - loss: 0.0549 - accuracy: 0.9841 - val_loss: 0.0057 - val_accuracy: 0.9985\n",
      "Epoch 19/50\n",
      "526/526 [==============================] - 68s 130ms/step - loss: 0.0490 - accuracy: 0.9852 - val_loss: 0.0042 - val_accuracy: 0.9991\n",
      "Epoch 20/50\n",
      "526/526 [==============================] - 68s 130ms/step - loss: 0.0506 - accuracy: 0.9846 - val_loss: 0.0037 - val_accuracy: 0.9991\n",
      "Epoch 21/50\n",
      "526/526 [==============================] - 83s 157ms/step - loss: 0.0574 - accuracy: 0.9831 - val_loss: 0.0058 - val_accuracy: 0.9985\n",
      "Epoch 22/50\n",
      "526/526 [==============================] - 86s 163ms/step - loss: 0.0480 - accuracy: 0.9862 - val_loss: 0.0042 - val_accuracy: 0.9987\n",
      "Epoch 23/50\n",
      "526/526 [==============================] - 86s 163ms/step - loss: 0.0473 - accuracy: 0.9857 - val_loss: 0.0033 - val_accuracy: 0.9991\n",
      "Epoch 24/50\n",
      "526/526 [==============================] - 85s 161ms/step - loss: 0.0467 - accuracy: 0.9865 - val_loss: 0.0046 - val_accuracy: 0.9990\n",
      "Epoch 25/50\n",
      "526/526 [==============================] - 85s 162ms/step - loss: 0.0439 - accuracy: 0.9875 - val_loss: 0.0046 - val_accuracy: 0.9988\n",
      "Epoch 26/50\n",
      "526/526 [==============================] - 79s 150ms/step - loss: 0.0473 - accuracy: 0.9859 - val_loss: 0.0031 - val_accuracy: 0.9990\n",
      "Epoch 27/50\n",
      "526/526 [==============================] - 76s 145ms/step - loss: 0.0472 - accuracy: 0.9862 - val_loss: 0.0033 - val_accuracy: 0.9991\n",
      "Epoch 28/50\n",
      "526/526 [==============================] - 77s 146ms/step - loss: 0.0380 - accuracy: 0.9892 - val_loss: 0.0068 - val_accuracy: 0.9981\n",
      "Epoch 29/50\n",
      "526/526 [==============================] - 77s 146ms/step - loss: 0.0426 - accuracy: 0.9884 - val_loss: 0.0036 - val_accuracy: 0.9991\n",
      "Epoch 30/50\n",
      "526/526 [==============================] - 76s 145ms/step - loss: 0.0459 - accuracy: 0.9862 - val_loss: 0.0033 - val_accuracy: 0.9991\n",
      "Epoch 31/50\n",
      "526/526 [==============================] - 76s 145ms/step - loss: 0.0446 - accuracy: 0.9874 - val_loss: 0.0063 - val_accuracy: 0.9981\n",
      "Epoch 32/50\n",
      "526/526 [==============================] - 77s 146ms/step - loss: 0.0454 - accuracy: 0.9874 - val_loss: 0.0040 - val_accuracy: 0.9992\n",
      "Epoch 33/50\n",
      "526/526 [==============================] - 76s 145ms/step - loss: 0.0374 - accuracy: 0.9892 - val_loss: 0.0051 - val_accuracy: 0.9993\n",
      "Epoch 34/50\n",
      "526/526 [==============================] - 15268s 29s/step - loss: 0.0429 - accuracy: 0.9871 - val_loss: 0.0042 - val_accuracy: 0.9992\n",
      "Epoch 35/50\n",
      "526/526 [==============================] - 83s 157ms/step - loss: 0.0377 - accuracy: 0.9887 - val_loss: 0.0065 - val_accuracy: 0.9981\n",
      "Epoch 36/50\n",
      "526/526 [==============================] - 79s 151ms/step - loss: 0.0362 - accuracy: 0.9897 - val_loss: 0.0045 - val_accuracy: 0.9991\n",
      "Epoch 37/50\n",
      "526/526 [==============================] - 78s 149ms/step - loss: 0.0356 - accuracy: 0.9904 - val_loss: 0.0040 - val_accuracy: 0.9989\n",
      "Epoch 38/50\n",
      "526/526 [==============================] - 74s 141ms/step - loss: 0.0367 - accuracy: 0.9896 - val_loss: 0.0047 - val_accuracy: 0.9988\n",
      "Epoch 39/50\n",
      "526/526 [==============================] - 78s 148ms/step - loss: 0.0450 - accuracy: 0.9873 - val_loss: 0.0037 - val_accuracy: 0.9991\n",
      "Epoch 40/50\n",
      "526/526 [==============================] - 78s 149ms/step - loss: 0.0476 - accuracy: 0.9872 - val_loss: 0.0066 - val_accuracy: 0.9983\n",
      "Epoch 41/50\n",
      "526/526 [==============================] - 76s 145ms/step - loss: 0.0401 - accuracy: 0.9881 - val_loss: 0.0050 - val_accuracy: 0.9985\n",
      "Epoch 42/50\n",
      "526/526 [==============================] - 74s 141ms/step - loss: 0.0317 - accuracy: 0.9906 - val_loss: 0.0029 - val_accuracy: 0.9995\n",
      "Epoch 43/50\n",
      "526/526 [==============================] - 115s 219ms/step - loss: 0.0368 - accuracy: 0.9894 - val_loss: 0.0044 - val_accuracy: 0.9990\n",
      "Epoch 44/50\n",
      "526/526 [==============================] - 137s 261ms/step - loss: 0.0353 - accuracy: 0.9900 - val_loss: 0.0048 - val_accuracy: 0.9989\n",
      "Epoch 45/50\n",
      "526/526 [==============================] - 130s 248ms/step - loss: 0.0310 - accuracy: 0.9911 - val_loss: 0.0041 - val_accuracy: 0.9994\n",
      "Epoch 46/50\n",
      "526/526 [==============================] - 145s 276ms/step - loss: 0.0378 - accuracy: 0.9893 - val_loss: 0.0033 - val_accuracy: 0.9995\n",
      "Epoch 47/50\n",
      "526/526 [==============================] - 136s 257ms/step - loss: 0.0353 - accuracy: 0.9903 - val_loss: 0.0037 - val_accuracy: 0.9987\n",
      "Epoch 48/50\n",
      "526/526 [==============================] - 144s 274ms/step - loss: 0.0360 - accuracy: 0.9905 - val_loss: 0.0054 - val_accuracy: 0.9989\n",
      "Epoch 49/50\n",
      "526/526 [==============================] - 131s 248ms/step - loss: 0.0455 - accuracy: 0.9883 - val_loss: 0.0058 - val_accuracy: 0.9990\n",
      "Epoch 50/50\n",
      "526/526 [==============================] - 140s 267ms/step - loss: 0.0317 - accuracy: 0.9918 - val_loss: 0.0040 - val_accuracy: 0.9991\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "model.compile(optimizer='adam',loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),metrics=[\"accuracy\"])\n",
    "history = model.fit(aug_data.flow(train_x, train_y, batch_size=50), epochs = 50, validation_data = (val_x,val_y))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ff9c3df18c39e40a632fe67bd3f2effcf292bf7152cebacfc692487f5f951f29"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
