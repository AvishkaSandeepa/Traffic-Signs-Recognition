{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import thr required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Declare the directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory :  c:\\Users\\Avishka Sandeepa\\OneDrive - University of Moratuwa\\GitHub\\Traffic-Signs-Recognition\\main\\codes\n",
      "Current working directory :  C:\\Users\\Avishka Sandeepa\\OneDrive - University of Moratuwa\\GitHub\\Traffic-Signs-Recognition\\main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print (\"Current working directory : \" , current_directory)\n",
    "\n",
    "# define the directory of data set\n",
    "data_direcory = \"C:/Users/Avishka Sandeepa/OneDrive - University of Moratuwa/GitHub/Traffic-Signs-Recognition/main\"\n",
    "# chane the current directory to the data directory\n",
    "os.chdir(data_direcory)\n",
    "\n",
    "# chek the working directory again after changing\n",
    "current_directory = os.getcwd()\n",
    "print (\"Current working directory : \" , current_directory)\n",
    "\n",
    "# define the each folder directories for training and testing data\n",
    "train_set = 'Train'\n",
    "test_set = 'Test'\n",
    "\n",
    "num_classes = len(os.listdir(train_set)) # returns the no of classes inside the training folder(0 to 42)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preprocessing the Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acess the each and every image\n",
    "\n",
    "img_data =[]\n",
    "img_labels = []\n",
    " \n",
    "for index in range(num_classes):   # address all classes  \n",
    "    path = os.path.join(current_directory,'Train',str(index)) # create the path for each image    \n",
    "    image_names = os.listdir(path) # list containing names of all images for a running class at that point\n",
    "    \n",
    "    \n",
    "    for image in image_names:\n",
    "        img = cv.imread(path + '/' + image, cv.IMREAD_COLOR)\n",
    "        img = cv.resize(img, (32,32)) # resizing all images to one scale\n",
    "        # convert image data into numy array\n",
    "        img = np.array(img) \n",
    "\n",
    "        img_data.append(img)\n",
    "        img_labels.append(index)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39209, 32, 32, 3) (39209,)\n"
     ]
    }
   ],
   "source": [
    "# lets convert above created list into numpy arrays that helps to model to training\n",
    "\n",
    "img_data = np.array(img_data)\n",
    "img_labels = np.array(img_labels)\n",
    "\n",
    "print(img_data.shape, img_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape (26270, 32, 32, 3)\n",
      "val_x.shape (12939, 32, 32, 3)\n",
      "train_y.shape (26270,)\n",
      "val_y.shape (12939,)\n"
     ]
    }
   ],
   "source": [
    "# Split into training and validation data\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(img_data, img_labels, test_size=0.33, random_state=42, shuffle=True)\n",
    "train_x = train_x/255 \n",
    "val_x = val_x/255\n",
    "\n",
    "print(\"train_x.shape\", train_x.shape)\n",
    "print(\"val_x.shape\", val_x.shape)\n",
    "print(\"train_y.shape\", train_y.shape)\n",
    "print(\"val_y.shape\", val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y.shape (26270, 43)\n",
      "val_y.shape (12939, 43)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#This function returns a matrix of binary values (either ‘1’ or ‘0’). It has number of rows equal to the length of\n",
    "#the input vector and number of columns equal to the number of classes.\n",
    "train_y = tf.keras.utils.to_categorical(train_y, num_classes=num_classes)\n",
    "val_y = tf.keras.utils.to_categorical(val_y, num_classes=num_classes)\n",
    "\n",
    "print(\"train_y.shape\", train_y.shape)\n",
    "print(\"val_y.shape\", val_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Augmenting the image data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_data = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    fill_mode=\"nearest\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Build the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 30, 30, 16)        448       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 28, 28, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 12, 12, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 10, 10, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               1638912   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 43)                22059     \n",
      "=================================================================\n",
      "Total params: 1,758,411\n",
      "Trainable params: 1,758,411\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), activation = \"relu\", input_shape=(32,32,3)))\n",
    "model.add(Conv2D(32, (3, 3), activation = \"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) # maxpooling to reduce the dimentions\n",
    "model.add(Dropout(rate=0.25)) # Add dropout to prevent overfitting \n",
    "\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation = \"relu\"))\n",
    "model.add(Conv2D(128, (3, 3), activation = \"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(rate=0.25)) # Add dropout to prevent overfitting \n",
    "\n",
    "\n",
    "# Flatten serves as a connection between the convolution and dense layers. \n",
    "model.add(Flatten()) \n",
    "model.add(Dense(512, activation = \"relu\"))\n",
    "model.add(Dropout(rate=0.5)) # Add dropout to prevent overfitting \n",
    "\n",
    "# since we have 43 classes add dense with 43 \n",
    "model.add(Dense(43, activation = \"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "526/526 [==============================] - 37s 70ms/step - loss: 3.0757 - accuracy: 0.1939 - val_loss: 2.3298 - val_accuracy: 0.4057\n",
      "Epoch 2/50\n",
      "526/526 [==============================] - 40s 76ms/step - loss: 2.2631 - accuracy: 0.4022 - val_loss: 1.6186 - val_accuracy: 0.6051\n",
      "Epoch 3/50\n",
      "526/526 [==============================] - 38s 73ms/step - loss: 1.8289 - accuracy: 0.5250 - val_loss: 1.2923 - val_accuracy: 0.7033\n",
      "Epoch 4/50\n",
      "526/526 [==============================] - 42s 80ms/step - loss: 1.5595 - accuracy: 0.6003 - val_loss: 1.0669 - val_accuracy: 0.7659\n",
      "Epoch 5/50\n",
      "526/526 [==============================] - 43s 81ms/step - loss: 1.3708 - accuracy: 0.6539 - val_loss: 0.9106 - val_accuracy: 0.7868\n",
      "Epoch 6/50\n",
      "526/526 [==============================] - 42s 80ms/step - loss: 1.2566 - accuracy: 0.6822 - val_loss: 0.8336 - val_accuracy: 0.8057\n",
      "Epoch 7/50\n",
      "526/526 [==============================] - 47s 89ms/step - loss: 1.1832 - accuracy: 0.7045 - val_loss: 0.7902 - val_accuracy: 0.8129\n",
      "Epoch 8/50\n",
      "526/526 [==============================] - 47s 88ms/step - loss: 1.1270 - accuracy: 0.7156 - val_loss: 0.7512 - val_accuracy: 0.8179\n",
      "Epoch 9/50\n",
      "526/526 [==============================] - 47s 89ms/step - loss: 1.0809 - accuracy: 0.7276 - val_loss: 0.7444 - val_accuracy: 0.8151\n",
      "Epoch 10/50\n",
      "526/526 [==============================] - 42s 79ms/step - loss: 1.0533 - accuracy: 0.7333 - val_loss: 0.7139 - val_accuracy: 0.8254\n",
      "Epoch 11/50\n",
      "526/526 [==============================] - 42s 80ms/step - loss: 1.0250 - accuracy: 0.7408 - val_loss: 0.7110 - val_accuracy: 0.8257\n",
      "Epoch 12/50\n",
      "526/526 [==============================] - 42s 80ms/step - loss: 0.9968 - accuracy: 0.7485 - val_loss: 0.6678 - val_accuracy: 0.8392\n",
      "Epoch 13/50\n",
      "526/526 [==============================] - 41s 78ms/step - loss: 0.9427 - accuracy: 0.7606 - val_loss: 0.6263 - val_accuracy: 0.8476\n",
      "Epoch 14/50\n",
      "526/526 [==============================] - 40s 77ms/step - loss: 0.9030 - accuracy: 0.7728 - val_loss: 0.6093 - val_accuracy: 0.8490\n",
      "Epoch 15/50\n",
      "526/526 [==============================] - 38s 73ms/step - loss: 0.8906 - accuracy: 0.7713 - val_loss: 0.6036 - val_accuracy: 0.8505\n",
      "Epoch 16/50\n",
      "526/526 [==============================] - 39s 75ms/step - loss: 0.8706 - accuracy: 0.7779 - val_loss: 0.5996 - val_accuracy: 0.8507\n",
      "Epoch 17/50\n",
      "526/526 [==============================] - 40s 76ms/step - loss: 0.8567 - accuracy: 0.7817 - val_loss: 0.5937 - val_accuracy: 0.8501\n",
      "Epoch 18/50\n",
      "526/526 [==============================] - 41s 79ms/step - loss: 0.8412 - accuracy: 0.7841 - val_loss: 0.5840 - val_accuracy: 0.8532\n",
      "Epoch 19/50\n",
      "526/526 [==============================] - 44s 84ms/step - loss: 0.8281 - accuracy: 0.7880 - val_loss: 0.5819 - val_accuracy: 0.8529\n",
      "Epoch 20/50\n",
      "526/526 [==============================] - 43s 82ms/step - loss: 0.8181 - accuracy: 0.7904 - val_loss: 0.5812 - val_accuracy: 0.8533\n",
      "Epoch 21/50\n",
      "526/526 [==============================] - 45s 86ms/step - loss: 0.8087 - accuracy: 0.7935 - val_loss: 0.5746 - val_accuracy: 0.8552\n",
      "Epoch 22/50\n",
      "526/526 [==============================] - 45s 85ms/step - loss: 0.8072 - accuracy: 0.7940 - val_loss: 0.5777 - val_accuracy: 0.8525\n",
      "Epoch 23/50\n",
      "526/526 [==============================] - 42s 80ms/step - loss: 0.7918 - accuracy: 0.7979 - val_loss: 0.5694 - val_accuracy: 0.8551\n",
      "Epoch 24/50\n",
      "526/526 [==============================] - 43s 82ms/step - loss: 0.7917 - accuracy: 0.7974 - val_loss: 0.5695 - val_accuracy: 0.8544\n",
      "Epoch 25/50\n",
      "526/526 [==============================] - 42s 80ms/step - loss: 0.7894 - accuracy: 0.7990 - val_loss: 0.5641 - val_accuracy: 0.8555\n",
      "Epoch 26/50\n",
      "526/526 [==============================] - 47s 88ms/step - loss: 0.7791 - accuracy: 0.7985 - val_loss: 0.5627 - val_accuracy: 0.8572\n",
      "Epoch 27/50\n",
      "526/526 [==============================] - 48s 92ms/step - loss: 0.7690 - accuracy: 0.8037 - val_loss: 0.5632 - val_accuracy: 0.8558\n",
      "Epoch 28/50\n",
      "526/526 [==============================] - 47s 90ms/step - loss: 0.7615 - accuracy: 0.8049 - val_loss: 0.5630 - val_accuracy: 0.8569\n",
      "Epoch 29/50\n",
      "526/526 [==============================] - 60s 114ms/step - loss: 0.7754 - accuracy: 0.8022 - val_loss: 0.5618 - val_accuracy: 0.8573\n",
      "Epoch 30/50\n",
      "526/526 [==============================] - 50s 95ms/step - loss: 0.7601 - accuracy: 0.8060 - val_loss: 0.5605 - val_accuracy: 0.8562\n",
      "Epoch 31/50\n",
      "526/526 [==============================] - 54s 102ms/step - loss: 0.7529 - accuracy: 0.8077 - val_loss: 0.5590 - val_accuracy: 0.8568\n",
      "Epoch 32/50\n",
      "526/526 [==============================] - 47s 89ms/step - loss: 0.7578 - accuracy: 0.8076 - val_loss: 0.5606 - val_accuracy: 0.8567\n",
      "Epoch 33/50\n",
      "526/526 [==============================] - 48s 91ms/step - loss: 0.7485 - accuracy: 0.8103 - val_loss: 0.5550 - val_accuracy: 0.8574\n",
      "Epoch 34/50\n",
      "526/526 [==============================] - 52s 100ms/step - loss: 0.7461 - accuracy: 0.8099 - val_loss: 0.5583 - val_accuracy: 0.8566\n",
      "Epoch 35/50\n",
      "526/526 [==============================] - 45s 85ms/step - loss: 0.7532 - accuracy: 0.8083 - val_loss: 0.5611 - val_accuracy: 0.8570\n",
      "Epoch 36/50\n",
      "526/526 [==============================] - 49s 93ms/step - loss: 0.7488 - accuracy: 0.8087 - val_loss: 0.5574 - val_accuracy: 0.8574\n",
      "Epoch 37/50\n",
      "526/526 [==============================] - 46s 87ms/step - loss: 0.7385 - accuracy: 0.8112 - val_loss: 0.5537 - val_accuracy: 0.8576\n",
      "Epoch 38/50\n",
      "526/526 [==============================] - 43s 81ms/step - loss: 0.7339 - accuracy: 0.8137 - val_loss: 0.5557 - val_accuracy: 0.8576\n",
      "Epoch 39/50\n",
      "526/526 [==============================] - 46s 87ms/step - loss: 0.7357 - accuracy: 0.8125 - val_loss: 0.5548 - val_accuracy: 0.8578\n",
      "Epoch 40/50\n",
      "526/526 [==============================] - 50s 95ms/step - loss: 0.7439 - accuracy: 0.8083 - val_loss: 0.5539 - val_accuracy: 0.8579\n",
      "Epoch 41/50\n",
      "526/526 [==============================] - 39s 74ms/step - loss: 0.7337 - accuracy: 0.8134 - val_loss: 0.5572 - val_accuracy: 0.8566\n",
      "Epoch 42/50\n",
      "526/526 [==============================] - 40s 77ms/step - loss: 0.7270 - accuracy: 0.8135 - val_loss: 0.5535 - val_accuracy: 0.8569\n",
      "Epoch 43/50\n",
      "526/526 [==============================] - 43s 81ms/step - loss: 0.7262 - accuracy: 0.8152 - val_loss: 0.5541 - val_accuracy: 0.8574\n",
      "Epoch 44/50\n",
      "526/526 [==============================] - 39s 74ms/step - loss: 0.7238 - accuracy: 0.8144 - val_loss: 0.5536 - val_accuracy: 0.8577\n",
      "Epoch 45/50\n",
      "526/526 [==============================] - 47s 89ms/step - loss: 0.7241 - accuracy: 0.8155 - val_loss: 0.5509 - val_accuracy: 0.8586\n",
      "Epoch 46/50\n",
      "526/526 [==============================] - 45s 86ms/step - loss: 0.7193 - accuracy: 0.8176 - val_loss: 0.5494 - val_accuracy: 0.8586\n",
      "Epoch 47/50\n",
      "526/526 [==============================] - 40s 76ms/step - loss: 0.7143 - accuracy: 0.8173 - val_loss: 0.5519 - val_accuracy: 0.8585\n",
      "Epoch 48/50\n",
      "526/526 [==============================] - 47s 89ms/step - loss: 0.7141 - accuracy: 0.8170 - val_loss: 0.5527 - val_accuracy: 0.8578\n",
      "Epoch 49/50\n",
      "526/526 [==============================] - 43s 81ms/step - loss: 0.7229 - accuracy: 0.8160 - val_loss: 0.5496 - val_accuracy: 0.8586\n",
      "Epoch 50/50\n",
      "526/526 [==============================] - 44s 84ms/step - loss: 0.7206 - accuracy: 0.8158 - val_loss: 0.5522 - val_accuracy: 0.8584\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "model.compile(optimizer='adam',loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),metrics=[\"accuracy\"])\n",
    "history = model.fit(aug_data.flow(train_x, train_y, batch_size=50), epochs = 50, validation_data = (val_x,val_y))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ff9c3df18c39e40a632fe67bd3f2effcf292bf7152cebacfc692487f5f951f29"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
